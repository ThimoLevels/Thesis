{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to /private/var/folders/fw/5cvj544j10z207nylyr01rrm0000gn/T/pip-req-build-vr_0f91m\n",
      "  Running command git clone -q https://github.com/JustAnotherArchivist/snscrape.git /private/var/folders/fw/5cvj544j10z207nylyr01rrm0000gn/T/pip-req-build-vr_0f91m\n",
      "Requirement already satisfied: requests[socks] in ./opt/anaconda3/lib/python3.8/site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (2.24.0)\n",
      "Requirement already satisfied: lxml in ./opt/anaconda3/lib/python3.8/site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (4.5.2)\n",
      "Requirement already satisfied: beautifulsoup4 in ./opt/anaconda3/lib/python3.8/site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (4.9.1)\n",
      "Requirement already satisfied: pytz in ./opt/anaconda3/lib/python3.8/site-packages (from snscrape==0.3.5.dev96+g47fbc2a) (2020.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in ./opt/anaconda3/lib/python3.8/site-packages (from requests[socks]->snscrape==0.3.5.dev96+g47fbc2a) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->snscrape==0.3.5.dev96+g47fbc2a) (2.0.1)\n",
      "Building wheels for collected packages: snscrape\n",
      "  Building wheel for snscrape (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for snscrape: filename=snscrape-0.3.5.dev96+g47fbc2a-py3-none-any.whl size=50122 sha256=c0780b027cf4ca0ad7c2e35d6de631b29d68388f2fef9745e4013beff1a65168\n",
      "  Stored in directory: /private/var/folders/fw/5cvj544j10z207nylyr01rrm0000gn/T/pip-ephem-wheel-cache-evixlprc/wheels/92/42/87/33fa9b18f7a75d02643a9ca3743339aec9be28c6796267c7d8\n",
      "Successfully built snscrape\n",
      "Installing collected packages: snscrape\n",
      "Successfully installed snscrape-0.3.5.dev96+g47fbc2a\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using OS library to call CLI commands in Python\n",
    "os.system(\"snscrape --jsonl --max-results 100 twitter-search 'from:Thimoo_'> user-tweets.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a list with the dates for the first dataset (2011-2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011-01-03 2011-01-10\n",
      "2011-01-10 2011-01-17\n",
      "2011-01-17 2011-01-24\n",
      "2011-01-24 2011-01-31\n",
      "2011-01-31 2011-02-07\n",
      "2011-02-07 2011-02-14\n",
      "2011-02-14 2011-02-21\n",
      "2011-02-21 2011-02-28\n",
      "2011-02-28 2011-03-07\n",
      "2011-03-07 2011-03-14\n",
      "2011-03-14 2011-03-21\n",
      "2011-03-21 2011-03-28\n",
      "2011-03-28 2011-04-04\n",
      "2011-04-04 2011-04-11\n",
      "2011-04-11 2011-04-18\n",
      "2011-04-18 2011-04-25\n",
      "2011-04-25 2011-05-02\n",
      "2011-05-02 2011-05-09\n",
      "2011-05-09 2011-05-16\n",
      "2011-05-16 2011-05-23\n",
      "2011-05-23 2011-05-30\n",
      "2011-05-30 2011-06-06\n",
      "2011-06-06 2011-06-13\n",
      "2011-06-13 2011-06-20\n",
      "2011-06-20 2011-06-27\n",
      "2011-06-27 2011-07-04\n",
      "2011-07-04 2011-07-11\n",
      "2011-07-11 2011-07-18\n",
      "2011-07-18 2011-07-25\n",
      "2011-07-25 2011-08-01\n",
      "2011-08-01 2011-08-08\n",
      "2011-08-08 2011-08-15\n",
      "2011-08-15 2011-08-22\n",
      "2011-08-22 2011-08-29\n",
      "2011-08-29 2011-09-05\n",
      "2011-09-05 2011-09-12\n",
      "2011-09-12 2011-09-19\n",
      "2011-09-19 2011-09-26\n",
      "2011-09-26 2011-10-03\n",
      "2011-10-03 2011-10-10\n",
      "2011-10-10 2011-10-17\n",
      "2011-10-17 2011-10-24\n",
      "2011-10-24 2011-10-31\n",
      "2011-10-31 2011-11-07\n",
      "2011-11-07 2011-11-14\n",
      "2011-11-14 2011-11-21\n",
      "2011-11-21 2011-11-28\n",
      "2011-11-28 2011-12-05\n",
      "2011-12-05 2011-12-12\n",
      "2011-12-12 2011-12-19\n",
      "2011-12-19 2011-12-26\n",
      "2011-12-26 2012-01-02\n",
      "2012-01-02 2012-01-09\n",
      "2012-01-09 2012-01-16\n",
      "2012-01-16 2012-01-23\n",
      "2012-01-23 2012-01-30\n",
      "2012-01-30 2012-02-06\n",
      "2012-02-06 2012-02-13\n",
      "2012-02-13 2012-02-20\n",
      "2012-02-20 2012-02-27\n",
      "2012-02-27 2012-03-05\n",
      "2012-03-05 2012-03-12\n",
      "2012-03-12 2012-03-19\n",
      "2012-03-19 2012-03-26\n",
      "2012-03-26 2012-04-02\n",
      "2012-04-02 2012-04-09\n",
      "2012-04-09 2012-04-16\n",
      "2012-04-16 2012-04-23\n",
      "2012-04-23 2012-04-30\n",
      "2012-04-30 2012-05-07\n",
      "2012-05-07 2012-05-14\n",
      "2012-05-14 2012-05-21\n",
      "2012-05-21 2012-05-28\n",
      "2012-05-28 2012-06-04\n",
      "2012-06-04 2012-06-11\n",
      "2012-06-11 2012-06-18\n",
      "2012-06-18 2012-06-25\n",
      "2012-06-25 2012-07-02\n",
      "2012-07-02 2012-07-09\n",
      "2012-07-09 2012-07-16\n",
      "2012-07-16 2012-07-23\n",
      "2012-07-23 2012-07-30\n",
      "2012-07-30 2012-08-06\n",
      "2012-08-06 2012-08-13\n",
      "2012-08-13 2012-08-20\n",
      "2012-08-20 2012-08-27\n",
      "2012-08-27 2012-09-03\n",
      "2012-09-03 2012-09-10\n",
      "2012-09-10 2012-09-17\n",
      "2012-09-17 2012-09-24\n",
      "2012-09-24 2012-10-01\n",
      "2012-10-01 2012-10-08\n",
      "2012-10-08 2012-10-15\n",
      "2012-10-15 2012-10-22\n",
      "2012-10-22 2012-10-29\n",
      "2012-10-29 2012-11-05\n",
      "2012-11-05 2012-11-12\n",
      "2012-11-12 2012-11-19\n",
      "2012-11-19 2012-11-26\n",
      "2012-11-26 2012-12-03\n",
      "2012-12-03 2012-12-10\n",
      "2012-12-10 2012-12-17\n",
      "2012-12-17 2012-12-24\n",
      "2012-12-24 2012-12-31\n",
      "2012-12-31 2013-01-07\n",
      "2013-01-07 2013-01-14\n",
      "2013-01-14 2013-01-21\n",
      "2013-01-21 2013-01-28\n",
      "2013-01-28 2013-02-04\n",
      "2013-02-04 2013-02-11\n",
      "2013-02-11 2013-02-18\n",
      "2013-02-18 2013-02-25\n",
      "2013-02-25 2013-03-04\n",
      "2013-03-04 2013-03-11\n",
      "2013-03-11 2013-03-18\n",
      "2013-03-18 2013-03-25\n",
      "2013-03-25 2013-04-01\n",
      "2013-04-01 2013-04-08\n",
      "2013-04-08 2013-04-15\n",
      "2013-04-15 2013-04-22\n",
      "2013-04-22 2013-04-29\n",
      "2013-04-29 2013-05-06\n",
      "2013-05-06 2013-05-13\n",
      "2013-05-13 2013-05-20\n",
      "2013-05-20 2013-05-27\n",
      "2013-05-27 2013-06-03\n",
      "2013-06-03 2013-06-10\n",
      "2013-06-10 2013-06-17\n",
      "2013-06-17 2013-06-24\n",
      "2013-06-24 2013-07-01\n",
      "2013-07-01 2013-07-08\n",
      "2013-07-08 2013-07-15\n",
      "2013-07-15 2013-07-22\n",
      "2013-07-22 2013-07-29\n",
      "2013-07-29 2013-08-05\n",
      "2013-08-05 2013-08-12\n",
      "2013-08-12 2013-08-19\n",
      "2013-08-19 2013-08-26\n",
      "2013-08-26 2013-09-02\n",
      "2013-09-02 2013-09-09\n",
      "2013-09-09 2013-09-16\n",
      "2013-09-16 2013-09-23\n",
      "2013-09-23 2013-09-30\n",
      "2013-09-30 2013-10-07\n",
      "2013-10-07 2013-10-14\n",
      "2013-10-14 2013-10-21\n",
      "2013-10-21 2013-10-28\n",
      "2013-10-28 2013-11-04\n",
      "2013-11-04 2013-11-11\n",
      "2013-11-11 2013-11-18\n",
      "2013-11-18 2013-11-25\n",
      "2013-11-25 2013-12-02\n",
      "2013-12-02 2013-12-09\n",
      "2013-12-09 2013-12-16\n",
      "2013-12-16 2013-12-23\n",
      "2013-12-23 2013-12-30\n",
      "2013-12-30 2014-01-06\n",
      "2014-01-06 2014-01-13\n",
      "2014-01-13 2014-01-20\n",
      "2014-01-20 2014-01-27\n",
      "2014-01-27 2014-02-03\n",
      "2014-02-03 2014-02-10\n",
      "2014-02-10 2014-02-17\n",
      "2014-02-17 2014-02-24\n",
      "2014-02-24 2014-03-03\n",
      "2014-03-03 2014-03-10\n",
      "2014-03-10 2014-03-17\n",
      "2014-03-17 2014-03-24\n",
      "2014-03-24 2014-03-31\n",
      "2014-03-31 2014-04-07\n",
      "2014-04-07 2014-04-14\n",
      "2014-04-14 2014-04-21\n",
      "2014-04-21 2014-04-28\n",
      "2014-04-28 2014-05-05\n",
      "2014-05-05 2014-05-12\n",
      "2014-05-12 2014-05-19\n",
      "2014-05-19 2014-05-26\n",
      "2014-05-26 2014-06-02\n",
      "2014-06-02 2014-06-09\n",
      "2014-06-09 2014-06-16\n",
      "2014-06-16 2014-06-23\n",
      "2014-06-23 2014-06-30\n",
      "2014-06-30 2014-07-07\n",
      "2014-07-07 2014-07-14\n",
      "2014-07-14 2014-07-21\n",
      "2014-07-21 2014-07-28\n",
      "2014-07-28 2014-08-04\n",
      "2014-08-04 2014-08-11\n",
      "2014-08-11 2014-08-18\n",
      "2014-08-18 2014-08-25\n",
      "2014-08-25 2014-09-01\n",
      "2014-09-01 2014-09-08\n",
      "2014-09-08 2014-09-15\n",
      "2014-09-15 2014-09-22\n",
      "2014-09-22 2014-09-29\n",
      "2014-09-29 2014-10-06\n",
      "2014-10-06 2014-10-13\n",
      "2014-10-13 2014-10-20\n",
      "2014-10-20 2014-10-27\n",
      "2014-10-27 2014-11-03\n",
      "2014-11-03 2014-11-10\n",
      "2014-11-10 2014-11-17\n",
      "2014-11-17 2014-11-24\n",
      "2014-11-24 2014-12-01\n",
      "2014-12-01 2014-12-08\n",
      "2014-12-08 2014-12-15\n",
      "2014-12-15 2014-12-22\n",
      "2014-12-22 2014-12-29\n",
      "2014-12-29 2015-01-05\n",
      "2014-01-05 2015-01-12\n",
      "2015-01-12 2015-01-19\n",
      "2015-01-19 2015-01-26\n",
      "2015-01-26 2015-02-02\n",
      "2015-02-02 2015-02-09\n",
      "2015-02-09 2015-02-16\n",
      "2015-02-16 2015-02-23\n",
      "2015-02-23 2015-03-02\n",
      "2015-03-02 2015-03-09\n",
      "2015-03-09 2015-03-16\n",
      "2015-03-16 2015-03-23\n",
      "2015-03-23 2015-03-30\n",
      "2015-03-30 2015-04-06\n",
      "2015-04-06 2015-04-13\n",
      "2015-04-13 2015-04-20\n",
      "2015-04-20 2015-04-27\n",
      "2015-04-27 2015-05-04\n",
      "2015-05-04 2015-05-11\n",
      "2015-05-11 2015-05-18\n",
      "2015-05-18 2015-05-25\n",
      "2015-05-25 2015-06-01\n",
      "2015-06-01 2015-06-08\n",
      "2015-06-08 2015-06-15\n",
      "2015-06-15 2015-06-22\n",
      "2015-06-22 2015-06-29\n",
      "2015-06-29 2015-07-06\n",
      "2015-07-06 2015-07-13\n",
      "2015-07-13 2015-07-20\n",
      "2015-07-20 2015-07-27\n",
      "2015-07-27 2015-08-03\n",
      "2015-08-03 2015-08-10\n",
      "2015-08-10 2015-08-17\n",
      "2015-08-17 2015-08-24\n",
      "2015-08-24 2015-08-31\n",
      "2015-08-31 2015-09-07\n",
      "2015-09-07 2015-09-14\n",
      "2015-09-14 2015-09-21\n",
      "2015-09-21 2015-09-28\n",
      "2015-09-28 2015-10-05\n",
      "2015-10-05 2015-10-12\n",
      "2015-10-12 2015-10-19\n",
      "2015-10-19 2015-10-26\n",
      "2015-10-26 2015-11-02\n",
      "2015-11-02 2015-11-09\n",
      "2015-11-09 2015-11-16\n",
      "2015-11-16 2015-11-23\n",
      "2015-11-23 2015-11-30\n",
      "2015-11-30 2015-12-07\n",
      "2015-12-07 2015-12-14\n",
      "2015-12-14 2015-12-21\n",
      "2015-12-21 2015-12-28\n",
      "2015-12-28 2016-01-04\n",
      "2015-01-04 2016-01-11\n"
     ]
    }
   ],
   "source": [
    "years = ['2011', '2012', '2013', '2014', '2015']\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '01']\n",
    "\n",
    "\n",
    "start_day = 3\n",
    "\n",
    "dates = []\n",
    "\n",
    "for year in years:\n",
    "        count = 0\n",
    "        for i in range(1, 54):\n",
    "            end_day = start_day + 7\n",
    "            if end_day > 31 and (months[count] == '01' or months[count] == '03' or months[count] == '05' or months[count] == '07' or months[count] == '08' or months[count] == '10' or months[count] == '12'):\n",
    "                end_day = end_day - 31\n",
    "                start_date = months[count] + '-' + str(str(start_day).zfill(2))   \n",
    "                end_date = months[count+1] + '-' + str(str(end_day).zfill(2))\n",
    "            elif end_day > 30 and (months[count] == '04' or months[count] == '06' or months[count] == '09' or months[count] == '11'):\n",
    "                end_day = end_day - 30\n",
    "                start_date = months[count] + '-' + str(str(start_day).zfill(2)) \n",
    "                end_date = months[count+1] + '-' + str(str(end_day).zfill(2)) \n",
    "            elif end_day > 29 and months[count] == '02' and year == '2012':\n",
    "                end_day = end_day - 29\n",
    "                start_date = months[count] + '-' + str(str(start_day).zfill(2)) \n",
    "                end_date = months[count+1] + '-' + str(str(end_day).zfill(2))\n",
    "            elif end_day > 28 and months[count] == '02' and year != '2012':\n",
    "                end_day = end_day - 28\n",
    "                start_date = months[count] + '-' + str(str(start_day).zfill(2))  \n",
    "                end_date = months[count+1] + '-' + str(str(end_day).zfill(2)) \n",
    "            else: \n",
    "                start_date = months[count] + '-' + str(str(start_day).zfill(2))  \n",
    "                end_date = months[count] + '-' + str(str(end_day).zfill(2)) \n",
    "               \n",
    "            if count == 11 and start_day >= 25:\n",
    "                year_1 = str(int(year) + 1)\n",
    "            else:\n",
    "                year_1 = year\n",
    "            \n",
    "            if year == '2015' and i == 51:\n",
    "                break\n",
    "            \n",
    "            if i > 1:\n",
    "                if int(prev_year) > int(year_1):\n",
    "                    year_1 = prev_year\n",
    "            if i > 51:\n",
    "                year = prev_year\n",
    "                    \n",
    "            start_date = year + '-' + start_date\n",
    "            end_date = year_1 + '-' + end_date\n",
    "            print(start_date, end_date)\n",
    "            dates.append([start_date, end_date])\n",
    "                \n",
    "            start_day = start_day + 7\n",
    "            if start_day > 31 and (months[count] == '01' or months[count] == '03' or months[count] == '05' or months[count] == '07' or months[count] == '08' or months[count] == '10' or months[count] == '12'):\n",
    "                start_day = start_day - 31\n",
    "                count += 1\n",
    "            elif start_day > 30 and (months[count] == '04' or months[count] == '06' or months[count] == '09' or months[count] == '11'):\n",
    "                start_day = start_day - 30\n",
    "                count += 1\n",
    "            elif start_day > 29 and months[count] == '02' and year == '2012':\n",
    "                start_day = start_day - 29\n",
    "                count += 1\n",
    "            elif start_day > 28 and (months[count] == '02' and year != '2012'):\n",
    "                start_day = start_day - 28\n",
    "                count += 1\n",
    "            \n",
    "            prev_year = year_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2014-12-29', '2015-01-05'], ['2014-01-05', '2015-01-12'], ['2015-01-12', '2015-01-19'], ['2015-01-19', '2015-01-26'], ['2015-01-26', '2015-02-02'], ['2015-02-02', '2015-02-09'], ['2015-02-09', '2015-02-16'], ['2015-02-16', '2015-02-23'], ['2015-02-23', '2015-03-02'], ['2015-03-02', '2015-03-09'], ['2015-03-09', '2015-03-16'], ['2015-03-16', '2015-03-23'], ['2015-03-23', '2015-03-30'], ['2015-03-30', '2015-04-06'], ['2015-04-06', '2015-04-13'], ['2015-04-13', '2015-04-20'], ['2015-04-20', '2015-04-27'], ['2015-04-27', '2015-05-04'], ['2015-05-04', '2015-05-11'], ['2015-05-11', '2015-05-18'], ['2015-05-18', '2015-05-25'], ['2015-05-25', '2015-06-01'], ['2015-06-01', '2015-06-08'], ['2015-06-08', '2015-06-15'], ['2015-06-15', '2015-06-22'], ['2015-06-22', '2015-06-29'], ['2015-06-29', '2015-07-06'], ['2015-07-06', '2015-07-13'], ['2015-07-13', '2015-07-20'], ['2015-07-20', '2015-07-27'], ['2015-07-27', '2015-08-03'], ['2015-08-03', '2015-08-10'], ['2015-08-10', '2015-08-17'], ['2015-08-17', '2015-08-24'], ['2015-08-24', '2015-08-31'], ['2015-08-31', '2015-09-07'], ['2015-09-07', '2015-09-14'], ['2015-09-14', '2015-09-21'], ['2015-09-21', '2015-09-28'], ['2015-09-28', '2015-10-05'], ['2015-10-05', '2015-10-12'], ['2015-10-12', '2015-10-19'], ['2015-10-19', '2015-10-26'], ['2015-10-26', '2015-11-02'], ['2015-11-02', '2015-11-09'], ['2015-11-09', '2015-11-16'], ['2015-11-16', '2015-11-23'], ['2015-11-23', '2015-11-30'], ['2015-11-30', '2015-12-07'], ['2015-12-07', '2015-12-14'], ['2015-12-14', '2015-12-21'], ['2015-12-21', '2015-12-28'], ['2015-12-28', '2016-01-04']]\n"
     ]
    }
   ],
   "source": [
    "#print(dates)\n",
    "#print(dates[0:52]) #dates 2011\n",
    "#print(dates[52:104]) #dates 2012\n",
    "#print(dates[104:156]) #dates 2013\n",
    "#print(dates[156:208]) #dates 2014\n",
    "#print(dates[208:261]) #dates 2015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping oil companies for years 2011-2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 2011...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "years = ['2011', '2012', '2013', '2014', '2015']\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '01']\n",
    "oil_company = ['Aramco_Europe', 'GazpromEN', 'Shell', 'Total', 'exxonmobil', 'conocophillips', 'ONGC_', 'SinopecNews', 'Phillips66Co',\n",
    "               'ValeroEnergy', 'IndianOilcl', 'CanadianNatural', 'EnergyTransfer', 'Enbridge', 'omv',\n",
    "               'MarathonPetroCo', 'Suncor', 'bp_plc', 'Chevron', 'Equinor', 'cenovus', 'TCEnergy','official_cnpc', 'eni', 'DevonEnergy', 'WeAreOxy', 'RepsolWorldwide', 'NesteGlobal', 'MarathonOil', \n",
    "               'IEA', 'EIA', 'EIAgov', 'IOGP_News', 'IPIECA', 'OGAuthority', 'WoodsideEnergy',\n",
    "               'OilIndiaLimited']\n",
    "\n",
    "\n",
    "users = ['Aramco_Europe']\n",
    "basedir = '/Users/Thimo/Desktop/Data Thesis/Oil Companies/2011-2015'\n",
    "col_names = ['date', 'username', 'retweetCount', 'language', 'text']\n",
    "\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    year_directory = os.path.join(basedir, str(year)) \n",
    "\n",
    "    if not os.path.isdir(year_directory): \n",
    "        os.mkdir(year_directory)\n",
    "\n",
    "    if year == '2011':\n",
    "        print('Starting with 2011...')\n",
    "        for x in range(0, 52):\n",
    "            week_directory = os.path.join(year_directory, str(x+1))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for company in oil_company:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + company + ' ' + 'since:' + dates[x][0] + ' ' + 'until:' + dates[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%7 == 0:\n",
    "                time.sleep(600)\n",
    "\n",
    "    if year == '2012':\n",
    "        print('Starting with 2012...')\n",
    "        for x in range(52, 104):\n",
    "            week_directory = os.path.join(year_directory, str(x-43))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for company in oil_company:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + company + ' ' + 'since:' + dates[x][0] + ' ' + 'until:' + dates[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%7 == 0:\n",
    "                time.sleep(600)\n",
    "\n",
    "\n",
    "    if year == '2013':\n",
    "        print('Starting with 2013...')\n",
    "        for x in range(104, 156):\n",
    "            week_directory = os.path.join(year_directory, str(x-95))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for company in oil_company:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + company + ' ' + 'since:' + dates[x][0] + ' ' + 'until:' + dates[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',') \n",
    "            if x%7 == 0:\n",
    "                time.sleep(600)\n",
    "\n",
    "    if year == '2014':\n",
    "        print('Starting with 2014...')\n",
    "        for x in range(156, 208):\n",
    "            week_directory = os.path.join(year_directory, str(x-147))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for company in oil_company:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + company + ' ' + 'since:' + dates[x][0] + ' ' + 'until:' + dates[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%7 == 0:\n",
    "                time.sleep(600)\n",
    "\n",
    "    if year == '2015':\n",
    "        print('Starting with 2015...')\n",
    "        for x in range(208, 261):\n",
    "            week_directory = os.path.join(year_directory, str(x-199))\n",
    "            if not os.path.isdir(week_directory):\n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for company in oil_company:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + company + ' ' + 'since:' + dates[x][0] + ' ' + 'until:' + dates[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%7 == 0:\n",
    "                time.sleep(600)\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping think thanks for years 2011-2015 (working code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 2011...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "years = ['2011', '2012', '2013', '2014', '2015']\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '01']\n",
    "think_tanks = ['AfricaResearch', 'BAISC_int', 'ChathamHouse', 'DefenceSynergia', 'ecfr', 'FPCThinkTank',\n",
    "              'ISDglobal', 'IISS_org', 'lseideas', 'ODI_Global', 'PolarRPI', 'RUSI_org', 'TradBritGroup',\n",
    "              'vertic_org', 'WCIA_Wales', 'Bruegel_org', 'CEPS_thinktank', 'CER_EU', 'EgmontInstitute',\n",
    "              'EIASBrussels', 'EPLO_', 'epc_eu', 'EPIN_org', 'ESI_eu', 'South_Centre', 'lisboncouncil', \n",
    "              'IAIonline', 'EU_ISS', 'ISDP_Sweden', 'LEAP2020', 'elcanobrussels', 'Carnegie_Europe', \n",
    "              'ECDPM', 'ECIPE', 'ETUI_org', 'EU_ISS', 'FEPS_Europe', 'FriendsofEurope', \n",
    "              'instituteforgov', 'CrisisGroup', 'GPPi', 'eu_eeas', 'OSW_eng', 'EUROPEUMPrague', 'GFSIS_official',\n",
    "              'PISM_Poland', 'MartensCentre']\n",
    "basedir = '/Users/Thimo/Desktop/Data Thesis/Think Tanks/2011-2015'\n",
    "col_names = ['date', 'username', 'retweetCount', 'language', 'text']\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    year_directory = os.path.join(basedir, str(year)) \n",
    "\n",
    "    if not os.path.isdir(year_directory): \n",
    "        os.mkdir(year_directory)\n",
    "\n",
    "    if year == '2011':\n",
    "        print('Starting with 2011...')\n",
    "        for x in range(0, 52):\n",
    "            week_directory = os.path.join(year_directory, str(x+1))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for tt in think_tanks:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + tt + ' ' + 'since:' + dates[x][0] + ' ' + 'until:' + dates[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%5 == 0:\n",
    "                print(\"Time to sleep...\")\n",
    "                time.sleep(600) #Adding sleep time to limit the number of requests in a short time frame\n",
    "                print(\"Scraping continued...\")\n",
    "                \n",
    "\n",
    "    if year == '2012':\n",
    "        print('Starting with 2012...')\n",
    "        for x in range(52, 104):\n",
    "            week_directory = os.path.join(year_directory, str(x-43))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for tt in think_tanks:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + tt + ' ' + 'since:' + dates[x][0] + ' ' + 'until:' + dates[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%5 == 0:\n",
    "                print(\"Time to sleep...\")\n",
    "                time.sleep(600)\n",
    "                print(\"Scraping continued...\")\n",
    "\n",
    "\n",
    "    if year == '2013':\n",
    "        print('Starting with 2013...')\n",
    "        for x in range(104, 156):\n",
    "            week_directory = os.path.join(year_directory, str(x-95))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for tt in think_tanks:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + tt + ' ' + 'since:' + dates[x][0] + ' ' + 'until:' + dates[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%5 == 0:\n",
    "                print(\"Time to sleep...\")\n",
    "                time.sleep(600)\n",
    "                print(\"Scraping continued...\")\n",
    "\n",
    "    if year == '2014':\n",
    "        print('Starting with 2014...')\n",
    "        for x in range(156, 208):\n",
    "            week_directory = os.path.join(year_directory, str(x-147))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for tt in think_tanks:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + tt + ' ' + 'since:' + dates[x][0] + ' ' + 'until:' + dates[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%5 == 0:\n",
    "                print(\"Time to sleep...\")\n",
    "                time.sleep(600)\n",
    "                print(\"Scraping continued...\")\n",
    "\n",
    "    if year == '2015':\n",
    "        print('Starting with 2015...')\n",
    "        for x in range(208, 261):\n",
    "            week_directory = os.path.join(year_directory, str(x-199))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for tt in think_tanks:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + tt + ' ' + 'since:' + dates[x][0] + ' ' + 'until:' + dates[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%5 == 0:\n",
    "                print(\"Time to sleep...\")\n",
    "                time.sleep(600)\n",
    "                print(\"Scraping continued...\")\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a list with the dates for the expansion of the first dataset (2016-2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-04 2016-01-11\n",
      "2016-01-11 2016-01-18\n",
      "2016-01-18 2016-01-25\n",
      "2016-01-25 2016-02-01\n",
      "2016-02-01 2016-02-08\n",
      "2016-02-08 2016-02-15\n",
      "2016-02-15 2016-02-22\n",
      "2016-02-22 2016-02-29\n",
      "2016-02-29 2016-03-07\n",
      "2016-03-07 2016-03-14\n",
      "2016-03-14 2016-03-21\n",
      "2016-03-21 2016-03-28\n",
      "2016-03-28 2016-04-04\n",
      "2016-04-04 2016-04-11\n",
      "2016-04-11 2016-04-18\n",
      "2016-04-18 2016-04-25\n",
      "2016-04-25 2016-05-02\n",
      "2016-05-02 2016-05-09\n",
      "2016-05-09 2016-05-16\n",
      "2016-05-16 2016-05-23\n",
      "2016-05-23 2016-05-30\n",
      "2016-05-30 2016-06-06\n",
      "2016-06-06 2016-06-13\n",
      "2016-06-13 2016-06-20\n",
      "2016-06-20 2016-06-27\n",
      "2016-06-27 2016-07-04\n",
      "2016-07-04 2016-07-11\n",
      "2016-07-11 2016-07-18\n",
      "2016-07-18 2016-07-25\n",
      "2016-07-25 2016-08-01\n",
      "2016-08-01 2016-08-08\n",
      "2016-08-08 2016-08-15\n",
      "2016-08-15 2016-08-22\n",
      "2016-08-22 2016-08-29\n",
      "2016-08-29 2016-09-05\n",
      "2016-09-05 2016-09-12\n",
      "2016-09-12 2016-09-19\n",
      "2016-09-19 2016-09-26\n",
      "2016-09-26 2016-10-03\n",
      "2016-10-03 2016-10-10\n",
      "2016-10-10 2016-10-17\n",
      "2016-10-17 2016-10-24\n",
      "2016-10-24 2016-10-31\n",
      "2016-10-31 2016-11-07\n",
      "2016-11-07 2016-11-14\n",
      "2016-11-14 2016-11-21\n",
      "2016-11-21 2016-11-28\n",
      "2016-11-28 2016-12-05\n",
      "2016-12-05 2016-12-12\n",
      "2016-12-12 2016-12-19\n",
      "2016-12-19 2016-12-26\n",
      "2016-12-26 2017-01-02\n",
      "2017-01-02 2017-01-09\n",
      "2017-01-09 2017-01-16\n",
      "2017-01-16 2017-01-23\n",
      "2017-01-23 2017-01-30\n",
      "2017-01-30 2017-02-06\n",
      "2017-02-06 2017-02-13\n",
      "2017-02-13 2017-02-20\n",
      "2017-02-20 2017-02-27\n",
      "2017-02-27 2017-03-06\n",
      "2017-03-06 2017-03-13\n",
      "2017-03-13 2017-03-20\n",
      "2017-03-20 2017-03-27\n",
      "2017-03-27 2017-04-03\n",
      "2017-04-03 2017-04-10\n",
      "2017-04-10 2017-04-17\n",
      "2017-04-17 2017-04-24\n",
      "2017-04-24 2017-05-01\n",
      "2017-05-01 2017-05-08\n",
      "2017-05-08 2017-05-15\n",
      "2017-05-15 2017-05-22\n",
      "2017-05-22 2017-05-29\n",
      "2017-05-29 2017-06-05\n",
      "2017-06-05 2017-06-12\n",
      "2017-06-12 2017-06-19\n",
      "2017-06-19 2017-06-26\n",
      "2017-06-26 2017-07-03\n",
      "2017-07-03 2017-07-10\n",
      "2017-07-10 2017-07-17\n",
      "2017-07-17 2017-07-24\n",
      "2017-07-24 2017-07-31\n",
      "2017-07-31 2017-08-07\n",
      "2017-08-07 2017-08-14\n",
      "2017-08-14 2017-08-21\n",
      "2017-08-21 2017-08-28\n",
      "2017-08-28 2017-09-04\n",
      "2017-09-04 2017-09-11\n",
      "2017-09-11 2017-09-18\n",
      "2017-09-18 2017-09-25\n",
      "2017-09-25 2017-10-02\n",
      "2017-10-02 2017-10-09\n",
      "2017-10-09 2017-10-16\n",
      "2017-10-16 2017-10-23\n",
      "2017-10-23 2017-10-30\n",
      "2017-10-30 2017-11-06\n",
      "2017-11-06 2017-11-13\n",
      "2017-11-13 2017-11-20\n",
      "2017-11-20 2017-11-27\n",
      "2017-11-27 2017-12-04\n",
      "2017-12-04 2017-12-11\n",
      "2017-12-11 2017-12-18\n",
      "2017-12-18 2017-12-25\n",
      "2017-12-25 2018-01-01\n",
      "2018-01-01 2018-01-08\n",
      "2018-01-08 2018-01-15\n",
      "2018-01-15 2018-01-22\n",
      "2018-01-22 2018-01-29\n",
      "2018-01-29 2018-02-05\n",
      "2018-02-05 2018-02-12\n",
      "2018-02-12 2018-02-19\n",
      "2018-02-19 2018-02-26\n",
      "2018-02-26 2018-03-05\n",
      "2018-03-05 2018-03-12\n",
      "2018-03-12 2018-03-19\n",
      "2018-03-19 2018-03-26\n",
      "2018-03-26 2018-04-02\n",
      "2018-04-02 2018-04-09\n",
      "2018-04-09 2018-04-16\n",
      "2018-04-16 2018-04-23\n",
      "2018-04-23 2018-04-30\n",
      "2018-04-30 2018-05-07\n",
      "2018-05-07 2018-05-14\n",
      "2018-05-14 2018-05-21\n",
      "2018-05-21 2018-05-28\n",
      "2018-05-28 2018-06-04\n",
      "2018-06-04 2018-06-11\n",
      "2018-06-11 2018-06-18\n",
      "2018-06-18 2018-06-25\n",
      "2018-06-25 2018-07-02\n",
      "2018-07-02 2018-07-09\n",
      "2018-07-09 2018-07-16\n",
      "2018-07-16 2018-07-23\n",
      "2018-07-23 2018-07-30\n",
      "2018-07-30 2018-08-06\n",
      "2018-08-06 2018-08-13\n",
      "2018-08-13 2018-08-20\n",
      "2018-08-20 2018-08-27\n",
      "2018-08-27 2018-09-03\n",
      "2018-09-03 2018-09-10\n",
      "2018-09-10 2018-09-17\n",
      "2018-09-17 2018-09-24\n",
      "2018-09-24 2018-10-01\n",
      "2018-10-01 2018-10-08\n",
      "2018-10-08 2018-10-15\n",
      "2018-10-15 2018-10-22\n",
      "2018-10-22 2018-10-29\n",
      "2018-10-29 2018-11-05\n",
      "2018-11-05 2018-11-12\n",
      "2018-11-12 2018-11-19\n",
      "2018-11-19 2018-11-26\n",
      "2018-11-26 2018-12-03\n",
      "2018-12-03 2018-12-10\n",
      "2018-12-10 2018-12-17\n",
      "2018-12-17 2018-12-24\n",
      "2018-12-24 2018-12-31\n",
      "2018-12-31 2019-01-07\n",
      "2019-01-07 2019-01-14\n",
      "2019-01-14 2019-01-21\n",
      "2019-01-21 2019-01-28\n",
      "2019-01-28 2019-02-04\n",
      "2019-02-04 2019-02-11\n",
      "2019-02-11 2019-02-18\n",
      "2019-02-18 2019-02-25\n",
      "2019-02-25 2019-03-04\n",
      "2019-03-04 2019-03-11\n",
      "2019-03-11 2019-03-18\n",
      "2019-03-18 2019-03-25\n",
      "2019-03-25 2019-04-01\n",
      "2019-04-01 2019-04-08\n",
      "2019-04-08 2019-04-15\n",
      "2019-04-15 2019-04-22\n",
      "2019-04-22 2019-04-29\n",
      "2019-04-29 2019-05-06\n",
      "2019-05-06 2019-05-13\n",
      "2019-05-13 2019-05-20\n",
      "2019-05-20 2019-05-27\n",
      "2019-05-27 2019-06-03\n",
      "2019-06-03 2019-06-10\n",
      "2019-06-10 2019-06-17\n",
      "2019-06-17 2019-06-24\n",
      "2019-06-24 2019-07-01\n",
      "2019-07-01 2019-07-08\n",
      "2019-07-08 2019-07-15\n",
      "2019-07-15 2019-07-22\n",
      "2019-07-22 2019-07-29\n",
      "2019-07-29 2019-08-05\n",
      "2019-08-05 2019-08-12\n",
      "2019-08-12 2019-08-19\n",
      "2019-08-19 2019-08-26\n",
      "2019-08-26 2019-09-02\n",
      "2019-09-02 2019-09-09\n",
      "2019-09-09 2019-09-16\n",
      "2019-09-16 2019-09-23\n",
      "2019-09-23 2019-09-30\n",
      "2019-09-30 2019-10-07\n",
      "2019-10-07 2019-10-14\n",
      "2019-10-14 2019-10-21\n",
      "2019-10-21 2019-10-28\n",
      "2019-10-28 2019-11-04\n",
      "2019-11-04 2019-11-11\n",
      "2019-11-11 2019-11-18\n",
      "2019-11-18 2019-11-25\n",
      "2019-11-25 2019-12-02\n",
      "2019-12-02 2019-12-09\n",
      "2019-12-09 2019-12-16\n",
      "2019-12-16 2019-12-23\n",
      "2019-12-23 2019-12-30\n",
      "2019-12-30 2020-01-06\n"
     ]
    }
   ],
   "source": [
    "years = ['2016', '2017', '2018', '2019']\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '01']\n",
    "\n",
    "\n",
    "start_day = 4\n",
    "\n",
    "dates2 = []\n",
    "\n",
    "for year in years:\n",
    "    count = 0\n",
    "    for i in range(1, 54):\n",
    "        end_day = start_day + 7\n",
    "        if end_day > 31 and (months[count] == '01' or months[count] == '03' or months[count] == '05' or months[count] == '07' or months[count] == '08' or months[count] == '10' or months[count] == '12'):\n",
    "            end_day = end_day - 31\n",
    "            start_date = months[count] + '-' + str(str(start_day).zfill(2))   \n",
    "            end_date = months[count+1] + '-' + str(str(end_day).zfill(2))\n",
    "        elif end_day > 30 and (months[count] == '04' or months[count] == '06' or months[count] == '09' or months[count] == '11'):\n",
    "            end_day = end_day - 30\n",
    "            start_date = months[count] + '-' + str(str(start_day).zfill(2)) \n",
    "            end_date = months[count+1] + '-' + str(str(end_day).zfill(2)) \n",
    "        elif end_day > 29 and months[count] == '02' and year == '2016':\n",
    "            #print(end_day)\n",
    "            end_day = end_day - 29\n",
    "            start_date = months[count] + '-' + str(str(start_day).zfill(2)) \n",
    "            end_date = months[count+1] + '-' + str(str(end_day).zfill(2))\n",
    "        elif end_day > 28 and months[count] == '02' and year != '2016':\n",
    "            end_day = end_day - 28\n",
    "            start_date = months[count] + '-' + str(str(start_day).zfill(2))  \n",
    "            end_date = months[count+1] + '-' + str(str(end_day).zfill(2)) \n",
    "        else: \n",
    "            start_date = months[count] + '-' + str(str(start_day).zfill(2))  \n",
    "            end_date = months[count] + '-' + str(str(end_day).zfill(2)) \n",
    "\n",
    "        if count == 11 and start_day >= 25:\n",
    "            year_1 = str(int(year) + 1)\n",
    "        else:\n",
    "            year_1 = year\n",
    "\n",
    "        if year == '2019' and i == 51:\n",
    "            break\n",
    "\n",
    "        if i > 1:\n",
    "            if int(prev_year) > int(year_1):\n",
    "                year_1 = prev_year\n",
    "        if i > 51:\n",
    "            year = prev_year\n",
    "\n",
    "        start_date = year + '-' + start_date\n",
    "        end_date = year_1 + '-' + end_date\n",
    "        print(start_date, end_date)\n",
    "        dates2.append([start_date, end_date])\n",
    "\n",
    "        start_day = start_day + 7\n",
    "        if start_day > 31 and (months[count] == '01' or months[count] == '03' or months[count] == '05' or months[count] == '07' or months[count] == '08' or months[count] == '10' or months[count] == '12'):\n",
    "            start_day = start_day - 31\n",
    "            count += 1\n",
    "        elif start_day > 30 and (months[count] == '04' or months[count] == '06' or months[count] == '09' or months[count] == '11'):\n",
    "            start_day = start_day - 30\n",
    "            count += 1\n",
    "        elif start_day > 29 and months[count] == '02' and year == '2016':\n",
    "            start_day = start_day - 29\n",
    "            count += 1\n",
    "        elif start_day > 28 and months[count] == '02' and year != '2016':\n",
    "            start_day = start_day - 28\n",
    "            count += 1\n",
    "\n",
    "        prev_year = year_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-06 2020-01-13\n",
      "2020-01-13 2020-01-20\n",
      "2020-01-20 2020-01-27\n",
      "2020-01-27 2020-02-03\n",
      "2020-02-03 2020-02-10\n",
      "2020-02-10 2020-02-17\n",
      "2020-02-17 2020-02-24\n",
      "2020-02-24 2020-03-02\n",
      "2020-03-02 2020-03-09\n",
      "2020-03-09 2020-03-16\n",
      "2020-03-16 2020-03-23\n",
      "2020-03-23 2020-03-30\n",
      "2020-03-30 2020-04-06\n",
      "2020-04-06 2020-04-13\n",
      "2020-04-13 2020-04-20\n",
      "2020-04-20 2020-04-27\n",
      "2020-04-27 2020-05-04\n",
      "2020-05-04 2020-05-11\n",
      "2020-05-11 2020-05-18\n",
      "2020-05-18 2020-05-25\n",
      "2020-05-25 2020-06-01\n",
      "2020-06-01 2020-06-08\n",
      "2020-06-08 2020-06-15\n",
      "2020-06-15 2020-06-22\n",
      "2020-06-22 2020-06-29\n",
      "2020-06-29 2020-07-06\n",
      "2020-07-06 2020-07-13\n",
      "2020-07-13 2020-07-20\n",
      "2020-07-20 2020-07-27\n",
      "2020-07-27 2020-08-03\n",
      "2020-08-03 2020-08-10\n",
      "2020-08-10 2020-08-17\n",
      "2020-08-17 2020-08-24\n",
      "2020-08-24 2020-08-31\n",
      "2020-08-31 2020-09-07\n",
      "2020-09-07 2020-09-14\n",
      "2020-09-14 2020-09-21\n",
      "2020-09-21 2020-09-28\n",
      "2020-09-28 2020-10-05\n",
      "2020-10-05 2020-10-12\n",
      "2020-10-12 2020-10-19\n",
      "2020-10-19 2020-10-26\n",
      "2020-10-26 2020-11-02\n",
      "2020-11-02 2020-11-09\n",
      "2020-11-09 2020-11-16\n",
      "2020-11-16 2020-11-23\n",
      "2020-11-23 2020-11-30\n",
      "2020-11-30 2020-12-07\n",
      "2020-12-07 2020-12-14\n",
      "2020-12-14 2020-12-21\n",
      "2020-12-21 2020-12-28\n",
      "2020-12-28 2021-01-04\n"
     ]
    }
   ],
   "source": [
    "years = ['2020']\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '01']\n",
    "\n",
    "\n",
    "start_day = 6\n",
    "\n",
    "dates3 = []\n",
    "\n",
    "for year in years:\n",
    "    count = 0\n",
    "    for i in range(1, 53):\n",
    "        end_day = start_day + 7\n",
    "        if end_day > 31 and (months[count] == '01' or months[count] == '03' or months[count] == '05' or months[count] == '07' or months[count] == '08' or months[count] == '10' or months[count] == '12'):\n",
    "            end_day = end_day - 31\n",
    "            start_date = months[count] + '-' + str(str(start_day).zfill(2))   \n",
    "            end_date = months[count+1] + '-' + str(str(end_day).zfill(2))\n",
    "        elif end_day > 30 and (months[count] == '04' or months[count] == '06' or months[count] == '09' or months[count] == '11'):\n",
    "            end_day = end_day - 30\n",
    "            start_date = months[count] + '-' + str(str(start_day).zfill(2)) \n",
    "            end_date = months[count+1] + '-' + str(str(end_day).zfill(2)) \n",
    "        elif end_day > 29 and months[count] == '02':\n",
    "            end_day = end_day - 29\n",
    "            start_date = months[count] + '-' + str(str(start_day).zfill(2)) \n",
    "            end_date = months[count+1] + '-' + str(str(end_day).zfill(2))\n",
    "        else: \n",
    "            start_date = months[count] + '-' + str(str(start_day).zfill(2))  \n",
    "            end_date = months[count] + '-' + str(str(end_day).zfill(2)) \n",
    "        \n",
    "        if count == 11 and start_day >= 25:\n",
    "            year_1 = str(int(year) + 1)\n",
    "        else:\n",
    "            year_1 = year\n",
    "        \n",
    "        start_date = year + '-' + start_date\n",
    "        end_date = year_1 + '-' + end_date\n",
    "        print(start_date, end_date)\n",
    "        dates3.append([start_date, end_date])\n",
    "        \n",
    "        start_day = start_day + 7\n",
    "        if start_day > 31 and (months[count] == '01' or months[count] == '03' or months[count] == '05' or months[count] == '07' or months[count] == '08' or months[count] == '10' or months[count] == '12'):\n",
    "            start_day = start_day - 31\n",
    "            count += 1\n",
    "        elif start_day > 30 and (months[count] == '04' or months[count] == '06' or months[count] == '09' or months[count] == '11'):\n",
    "            start_day = start_day - 30\n",
    "            count += 1\n",
    "        elif start_day > 29 and months[count] == '02':\n",
    "            start_day = start_day - 29\n",
    "            count += 1\n",
    "        \n",
    "        prev_year = year_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    }
   ],
   "source": [
    "dates4 = dates2 + dates3\n",
    "print(len(dates4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dates)\n",
    "#print(dates4[0:52]) #dates 2016\n",
    "#print(dates4[52:104]) #dates 2017\n",
    "#print(dates4[104:156]) #dates 2018\n",
    "#print(dates4[156:209]) #dates 2019\n",
    "#print(dates4[209:261]) #dates 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for years 2016-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 2019...\n",
      "Starting with 2020...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "years = ['2016', '2017','2018', '2019', '2020'] \n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '01']\n",
    "oil_company = ['Aramco_Europe', 'Aramco', 'GazpromEN', 'Shell', 'Total', 'exxonmobil', 'conocophillips', 'ONGC_', 'SinopecNews', 'Phillips66Co',\n",
    "               'ValeroEnergy', 'IndianOilcl', 'CanadianNatural', 'EnergyTransfer', 'Enbridge', 'omv',\n",
    "               'MarathonPetroCo', 'Suncor', 'bp_plc', 'Chevron', 'Equinor', 'cenovus', 'TCEnergy','official_cnpc', 'eni', \n",
    "               'DevonEnergy', 'WeAreOxy', 'RepsolWorldwide', 'NesteGlobal', 'MarathonOil', 'IEA', 'EIA', 'EIAgov', \n",
    "               'IOGP_News', 'IPIECA', 'OGAuthority', 'WoodsideEnergy','OilIndiaLimited']\n",
    "\n",
    "users = ['Aramco_Europe']\n",
    "basedir = '/Users/Thimo/Desktop/Data Thesis/Oil Companies/2016-2020'\n",
    "col_names = ['date', 'username', 'retweetCount', 'language', 'text']\n",
    "\n",
    "for year in years:\n",
    "    year_directory = os.path.join(basedir, str(year)) \n",
    "\n",
    "    if not os.path.isdir(year_directory): \n",
    "        os.mkdir(year_directory)\n",
    "\n",
    "    if year == '2016':\n",
    "        print('Starting with 2016...')\n",
    "        for x in range(0, 52):\n",
    "            week_directory = os.path.join(year_directory, str(x+1))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for company in oil_company:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + company + ' ' + 'since:' + dates4[x][0] + ' ' + 'until:' + dates4[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates4[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%7 == 0:\n",
    "                time.sleep(600)\n",
    "\n",
    "    if year == '2017':\n",
    "        print('Starting with 2017...')\n",
    "        for x in range(52, 104):\n",
    "            week_directory = os.path.join(year_directory, str(x-51))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for company in oil_company:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + company + ' ' + 'since:' + dates4[x][0] + ' ' + 'until:' + dates4[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates4[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%7 == 0:\n",
    "                time.sleep(600)\n",
    "\n",
    "\n",
    "    if year == '2018':\n",
    "        print('Starting with 2018...')\n",
    "        for x in range(104, 156): \n",
    "            week_directory = os.path.join(year_directory, str(x-103))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for company in oil_company:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + company + ' ' + 'since:' + dates4[x][0] + ' ' + 'until:' + dates4[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates4[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%7 == 0:\n",
    "                time.sleep(600)\n",
    "\n",
    "    if year == '2019':\n",
    "        print('Starting with 2019...')\n",
    "        for x in range(156, 209): \n",
    "            week_directory = os.path.join(year_directory, str(x-155))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for company in oil_company:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + company + ' ' + 'since:' + dates4[x][0] + ' ' + 'until:' + dates4[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates4[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%7 == 0:\n",
    "                time.sleep(600)\n",
    "\n",
    "    if year == '2020':\n",
    "        print('Starting with 2020...')\n",
    "        for x in range(209, 261):\n",
    "            week_directory = os.path.join(year_directory, str(x-208))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for company in oil_company:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + company + ' ' + 'since:' + dates4[x][0] + ' ' + 'until:' + dates4[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates4[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%7 == 0:\n",
    "                time.sleep(600)\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 2016...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Starting with 2017...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Starting with 2018...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Starting with 2019...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Starting with 2020...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Time to sleep...\n",
      "Scraping continued...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "years = ['2016', '2017', '2018', '2019', '2020']\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '01']\n",
    "think_tanks = ['AfricaResearch', 'BAISC_int', 'ChathamHouse', 'DefenceSynergia', 'ecfr', 'FPCThinkTank',\n",
    "              'ISDglobal', 'IISS_org', 'lseideas', 'ODI_Global', 'PolarRPI', 'RUSI_org', 'TradBritGroup',\n",
    "              'vertic_org', 'WCIA_Wales', 'Bruegel_org', 'CEPS_thinktank', 'CER_EU', 'EgmontInstitute',\n",
    "              'EIASBrussels', 'EPLO_', 'epc_eu', 'EPIN_org', 'ESI_eu', 'South_Centre', 'lisboncouncil', \n",
    "              'IAIonline', 'EU_ISS', 'ISDP_Sweden', 'LEAP2020', 'elcanobrussels', 'Carnegie_Europe', \n",
    "              'ECDPM', 'ECIPE', 'ETUI_org', 'EU_ISS', 'FEPS_Europe', 'FriendsofEurope', \n",
    "              'instituteforgov', 'CrisisGroup', 'GPPi', 'eu_eeas', 'OSW_eng', 'EUROPEUMPrague', 'GFSIS_official',\n",
    "              'PISM_Poland', 'MartensCentre']\n",
    "\n",
    "basedir = '/Users/Thimo/Desktop/Data Thesis/Think Tanks/2016-2020'\n",
    "col_names = ['date', 'username', 'retweetCount', 'language', 'text']\n",
    "\n",
    "for year in years:\n",
    "    year_directory = os.path.join(basedir, str(year)) \n",
    "\n",
    "    if not os.path.isdir(year_directory): \n",
    "        os.mkdir(year_directory)\n",
    "\n",
    "    if year == '2016':\n",
    "        print('Starting with 2016...')\n",
    "        for x in range(0, 52):\n",
    "            week_directory = os.path.join(year_directory, str(x+1))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for tt in think_tanks:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + tt + ' ' + 'since:' + dates4[x][0] + ' ' + 'until:' + dates4[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates4[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%5 == 0:\n",
    "                print('Time to sleep...')\n",
    "                time.sleep(600)\n",
    "                print('Scraping continued...')\n",
    "\n",
    "    if year == '2017':\n",
    "        print('Starting with 2017...')\n",
    "        for x in range(52, 104):\n",
    "            week_directory = os.path.join(year_directory, str(x-51))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for tt in think_tanks:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + tt + ' ' + 'since:' + dates4[x][0] + ' ' + 'until:' + dates4[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates4[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%5 == 0:\n",
    "                print('Time to sleep...')\n",
    "                time.sleep(600)\n",
    "                print('Scraping continued...')\n",
    "\n",
    "\n",
    "    if year == '2018':\n",
    "        print('Starting with 2018...')\n",
    "        for x in range(104, 156):\n",
    "            week_directory = os.path.join(year_directory, str(x-103))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for tt in think_tanks:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + tt + ' ' + 'since:' + dates4[x][0] + ' ' + 'until:' + dates4[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates4[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%10 == 0:\n",
    "                print('Time to sleep...')\n",
    "                time.sleep(600)\n",
    "                print('Scraping continued...')\n",
    "\n",
    "    if year == '2019':\n",
    "        print('Starting with 2019...')\n",
    "        for x in range(156, 209):\n",
    "            week_directory = os.path.join(year_directory, str(x-155))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for tt in think_tanks:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + tt + ' ' + 'since:' + dates4[x][0] + ' ' + 'until:' + dates4[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates4[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%5 == 0:\n",
    "                print('Time to sleep...')\n",
    "                time.sleep(600)\n",
    "                print('Scraping continued...')\n",
    "\n",
    "    if year == '2020':\n",
    "        print('Starting with 2020...')\n",
    "        for x in range(209, 261):\n",
    "            week_directory = os.path.join(year_directory, str(x-208))\n",
    "            if not os.path.isdir(week_directory): \n",
    "                os.mkdir(week_directory)\n",
    "            user_tweets = pd.DataFrame(columns = col_names)\n",
    "            for tt in think_tanks:\n",
    "                tweets = []\n",
    "                for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:' + tt + ' ' + 'since:' + dates4[x][0] + ' ' + 'until:' + dates4[x][1]).get_items()):\n",
    "                    tweets.append([tweet.date, tweet.user.username, tweet.retweetCount, tweet.lang, tweet.content])\n",
    "\n",
    "                for i in range(len(tweets)):\n",
    "                    d = {'date': tweets[i][0], 'username':tweets[i][1], 'retweetCount': tweets[i][2], 'language': tweets[i][3], 'text':tweets[i][4]}\n",
    "                    user_tweets = user_tweets.append(d, ignore_index = True)\n",
    "            f_name2 = dates4[x][0] + '.csv'\n",
    "            user_tweets.to_csv(os.path.join(week_directory, f_name2), sep = ',')\n",
    "            if x%5 == 0:\n",
    "                print('Time to sleep...')\n",
    "                time.sleep(600)\n",
    "                print('Scraping continued...')\n",
    "    \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
